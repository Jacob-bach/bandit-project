{"cells":[{"cell_type":"markdown","id":"07817029","metadata":{"panel-layout":{"height":352.9000244140625,"visible":true,"width":100},"id":"07817029"},"source":["### Bandit Environment\n","\n","the below code implements the k-armed bandit problem which navigates the exploration-exploitation dilemma:\n","- There are k slot machines standing in front of an agent.\n","- The agent has h turns, where they must pick a slot machine's arm to pull per turn.\n","- The slot machine either produces a 1 or 0 as the reward.\n","- Each slot machine has it's own reward probability distribution unknown to the agent.\n","- The goal is to maximize the total reward after h turns.\n","\n","The optimal strategy to the problem challenges the agent to strike a balance of exploring the individual probability distributions of the different machines (exploration) and maximizing profits based on the information acquired so far (exploitation)\n","\n","#### Properties:\n","- currently, only supporting a stationary environment. Future updates will occur to allow the option of a non-stationary environment.\n","    - (a stationary environment means that the underlying true probabilities do not change over time so in our case, a non-stationary environment would mean that with each turn. The arms underlying probabilities would shift over turns)\n","- As the case with the historical k-bandit problem, our output for a pull is either 'success' or 'failure', in the future there will be an option for a continous variable output so that more policies can be applicable to the bandit environment"]},{"cell_type":"code","execution_count":1,"id":"0ba0c77d","metadata":{"id":"0ba0c77d","executionInfo":{"status":"ok","timestamp":1737818880399,"user_tz":360,"elapsed":166,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}}},"outputs":[],"source":["import math\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"id":"2637b0da","metadata":{"scrolled":true,"id":"2637b0da","executionInfo":{"status":"ok","timestamp":1737818881066,"user_tz":360,"elapsed":447,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}}},"outputs":[],"source":["class Bandit:\n","\n","\n","    def __init__(self, arms, turns, seed=None, dist='uniform', dist_params=None):\n","        '''\n","        Initialize a k-armed bandit environment. world state is automatically updated when pull(arm_index) is used.\n","\n","        Parameters:\n","            arms (int): The number of arms (k) of the bandit\n","            turns (int): The total number of pulls (horizon)\n","            seed (int): random seed used for reproducibility (compare RL algorithms)\n","            dist (str): The distribution to generate true reward probabilities.\n","                    options: 'unform' (default) or 'beta'\n","            dist_params (dict, optional): Additional parameters for the distribution as needed.\n","                    For 'beta', you might use {'a' : 1, 'b' : 1}\n","        '''\n","        # set random seed\n","        if seed is not None:\n","            np.random.seed(seed)\n","\n","        # Validate input parameters\n","        if arms <= 0 or turns <= 0:\n","            raise ValueError(\"The number of arms and turns must be positive integers.\")\n","\n","        self.k = arms                     # Number of arms (k)\n","        self.h = turns                    # Total number of pulls (horizon)\n","        self.turn = 1                     # Current turn\n","        self.scores = []                  # Store payout of different RL strategies\n","\n","        # internal state:\n","        # Each element is a tuple (n, w): n = number of pulls, w = total reward for that arm\n","        # one tuple per arm\n","        self.state = [(0, 0) for _ in range(self.k)]\n","\n","        # History recording: will store (turn_number, arm_index, reward)\n","        self.history = []\n","\n","        # True reward probabilities (hidden from the learning agent)\n","        if dist == 'uniform':\n","\n","            # Sample true reward probabilities uniformly between 0 and 1\n","            self.true_probs = np.random.rand(self.k)\n","\n","        elif dist == 'beta':\n","            #Use a beta distribution, Set default Beta(1,1) if no parameters provided\n","            a = 1\n","            b = 1\n","            if dist_params is not None:\n","                a = dist_params.get('a', 1)\n","                b = dist_params.get('b', 1)\n","            self.true_probs = np.random.beta(a, b, size=self.k)\n","        else:\n","            raise ValueError(\"Distribution type not recognized. Use 'uniform' or 'beta'.\")\n","\n","    def reset(self):\n","        '''\n","        Reset the bandit's state and history to the initial condition.\n","        '''\n","        self.state = [(0, 0) for _ in range(self.k)]\n","        self.history = []\n","        self.turn = 1\n","\n","    def pull(self, arm_index):\n","        '''\n","        Simulate pulling a specific arm.\n","\n","        Parameters:\n","            arm_index (int): The index of the arm to pull.\n","\n","        Returns:\n","            reward (int): The reward obtained (1 for success, 0 for failure).\n","        '''\n","        #validate # of turns\n","        if not (1 <= self.turn <= self.h):\n","            raise ValueError(\"turn must be between 1 and h.\")\n","\n","        #validate arm index\n","        if not (0 <= arm_index < self.k):\n","            raise ValueError(\"arm_index must be between 0 and k-1.\")\n","\n","        #determine if the pull results is a success by using a bernoulli distribution\n","        prob_success = self.true_probs[arm_index]\n","        reward = np.random.binomial(1, prob_success)\n","\n","        #update the state for that arm\n","        n, w = self.state[arm_index]\n","        new_n = n + 1             # number of times arm has been pulled after this pull\n","        new_w = w + reward        # total reward for arm after this pull\n","        self.state[arm_index] = (new_n, new_w)\n","\n","        #record the pull in history (turn, arm_index, reward)\n","        self.history.append({\"turn\": self.turn, \"arm\": arm_index, \"payout\" : reward})\n","\n","        #print state after pull\n","        message = f\"Turn {self.turn}: Pulled arm {arm_index}. \"\n","        message += \"Result: SUCCESS. \" if reward else \"Result: FAILURE. \"\n","        message += f\"Arm {arm_index} state: (#_of_pulls: {new_n}, total_reward: {new_w}). \"\n","\n","        print(message)\n","        self.turn += 1\n","\n","        return reward\n","\n","    def get_state(self):\n","        \"\"\"\n","        Return the current state.\n","        \"\"\"\n","        return list(self.state)\n","\n","    def get_history(self):\n","        \"\"\"\n","        Return the history of pulls.\n","        - (turn, arm_index, reward)\n","        \"\"\"\n","        return list(self.history)\n","\n","    def store_strat(self, name):\n","        \"\"\"\n","        store payout of RL strategy on current bandit environment and calls get_scores()\n","        \"\"\"\n","        payout = sum([score[1] for score in self.state])\n","        self.scores.append((name, payout))\n","\n","        self.get_scores()                        #print performance of policy\n","        self.reset()                             #reset environment\n","\n","    def get_scores(self):\n","        \"\"\"\n","        prints the payouts of all RL strategies on current bandit environment\n","        \"\"\"\n","        print('---------------------------------------------------------------------------')\n","        for name, payout in self.scores:\n","            print(f'strategy - {name} | total payout - {payout} | total turns - {self.h}')"]},{"cell_type":"markdown","id":"fc6d0605","metadata":{"panel-layout":{"height":143.06666564941406,"visible":true,"width":100},"id":"fc6d0605"},"source":["### Example Usage of Bandit Class:\n","To showcase how to use the Bandit environment with a defined policy I created 'basic_greed()', a pure greedy policy that fully utilizes past experiences to dictate which arm to pull.\n","- The policy calculates the 'win-rate' (reward/pulls) of each arm and picks the highest\n","- When multiple arms have the highest winrate, a 'np.random.choice()' is made\n","- Due to the nature of the policy, the agent at first explores the different arms until it finds initial success, wherein the arm that yields success will consistently be chosen therafter"]},{"cell_type":"code","execution_count":3,"id":"e4f22c3a","metadata":{"panel-layout":{"height":0,"visible":true,"width":100},"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e4f22c3a","executionInfo":{"status":"ok","timestamp":1737818881068,"user_tz":360,"elapsed":119,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}},"outputId":"73171a37-5096-4a55-9c0d-f265e23574a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["True reward probabilities (hidden):\n","[0.36284521 0.37732775 0.10454926 0.06138408 0.69978668 0.04114688]\n","Turn 1: Pulled arm 5. Result: FAILURE. Arm 5 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 2: Pulled arm 1. Result: FAILURE. Arm 1 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 3: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 4: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 2, total_reward: 0). \n","Turn 5: Pulled arm 3. Result: FAILURE. Arm 3 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 6: Pulled arm 5. Result: FAILURE. Arm 5 state: (#_of_pulls: 2, total_reward: 0). \n","Turn 7: Pulled arm 1. Result: FAILURE. Arm 1 state: (#_of_pulls: 2, total_reward: 0). \n","Turn 8: Pulled arm 4. Result: FAILURE. Arm 4 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 9: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 3, total_reward: 1). \n","Turn 10: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 4, total_reward: 2). \n","Turn 11: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 5, total_reward: 2). \n","Turn 12: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 6, total_reward: 2). \n","Turn 13: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 7, total_reward: 3). \n","Turn 14: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 8, total_reward: 3). \n","Turn 15: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 9, total_reward: 3). \n","\n","Current state:\n","[(9, 3), (2, 0), (0, 0), (1, 0), (1, 0), (2, 0)]\n","\n","History:\n","[{'turn': 1, 'arm': 5, 'payout': 0}, {'turn': 2, 'arm': 1, 'payout': 0}, {'turn': 3, 'arm': 0, 'payout': 0}, {'turn': 4, 'arm': 0, 'payout': 0}, {'turn': 5, 'arm': 3, 'payout': 0}, {'turn': 6, 'arm': 5, 'payout': 0}, {'turn': 7, 'arm': 1, 'payout': 0}, {'turn': 8, 'arm': 4, 'payout': 0}, {'turn': 9, 'arm': 0, 'payout': 1}, {'turn': 10, 'arm': 0, 'payout': 1}, {'turn': 11, 'arm': 0, 'payout': 0}, {'turn': 12, 'arm': 0, 'payout': 0}, {'turn': 13, 'arm': 0, 'payout': 1}, {'turn': 14, 'arm': 0, 'payout': 0}, {'turn': 15, 'arm': 0, 'payout': 0}]\n","\n","Payout:\n","---------------------------------------------------------------------------\n","strategy - Basic Greed | total payout - 3 | total turns - 15\n"]}],"source":["def basic_greed(world):\n","    '''\n","    basic pure greedy policy that aims to always pick the highest percentage chance of payout based on past experience.\n","    if there exist multiple options, randomly choose one using random.choice()\n","\n","    purposefully leaving inherent randomness\n","\n","    Parameters:\n","        world (Bandit): the environment in which we are utilizing strategy\n","    '''\n","    perc_chance = {}\n","\n","    while world.turn <= world.h:                    # keep pulling till out of turns\n","\n","        curr_state = world.get_state()              # Update current state\n","\n","        for arm, (n, w) in enumerate(curr_state):   # calculate performance history of each arm - payout / #_of_pulls\n","            if n == 0:                              # Dont divide by 0\n","                perc_chance[arm] = 0\n","            else:\n","                perc_chance[arm] = w/n              # % chance of payout\n","\n","\n","        #filter arm(s) with highest chances\n","        high_chance_arms = [arm for arm, perc in perc_chance.items() if perc == max(perc_chance.values())]\n","\n","        #randomly choose arm out of highest_chances\n","        world.pull(np.random.choice(high_chance_arms))\n","\n","        #end of while loop\n","\n","    print(\"\\nCurrent state:\")\n","    print(bandit_env.get_state())\n","\n","    print(\"\\nHistory:\")\n","    print(bandit_env.get_history())\n","\n","    print(\"\\nPayout:\")\n","    world.store_strat(\"Basic Greed\")               #stores the performance of RL policy and resets environment\n","\n","bandit_env = Bandit(arms=6, turns=15, seed=3, dist='beta', dist_params={'a': 1, 'b' : 1})\n","\n","print(\"True reward probabilities (hidden):\")\n","print(bandit_env.true_probs)\n","\n","# Simulate pulling a few arms\n","basic_greed(bandit_env)\n"]},{"cell_type":"markdown","id":"e43dfca6-7cf6-4aef-8ba7-2a495b5bcf30","metadata":{"id":"e43dfca6-7cf6-4aef-8ba7-2a495b5bcf30"},"source":["### Upper-Confidence-Bound(UCB)\n","upper-bound-confidence interval for the bandit problem environment\n","A higher c value is used when exploration is more critical within an environment\n","- non-stationary environment\n","- large action space (many arms)\n","- uncertain environments where large variation in rewards\n","\n","This algorithm is used for **Decision Making Under Uncertainty**\n","### UCB formula\n","$$ a_t = \\mathop{\\text{arg max}}_{\\textbf{i}}\\left(\\hat{\\mu_{i}}+c*\\sqrt{\\frac{\\ln{t}}{n_i}}\\right) $$\n","- $a_t$ : The action (arm) chosen at time $t$\n","- $\\hat{\\mu_{i}}$ : The mean reward\n","- $t$ : current turn\n","- $n_i$ : number of times arm $i$ has been pulled\n","- $c$ : exploration parameter\n","\n","\n","### Future Considerations\n","This policy does not perform well in a stationary Environment. While the logic is sound, it needs to be furthur tested once a non-stationary environment option is implemented to bandit"]},{"cell_type":"code","execution_count":4,"id":"53816194-8782-4f52-943c-aea49c650d85","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"53816194-8782-4f52-943c-aea49c650d85","executionInfo":{"status":"ok","timestamp":1737818881069,"user_tz":360,"elapsed":101,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}},"outputId":"ffffd5e7-9df0-4d43-afde-6411af3e039e"},"outputs":[{"output_type":"stream","name":"stdout","text":["True reward probabilities (hidden):\n","[0.36284521 0.37732775 0.10454926 0.06138408 0.69978668 0.04114688]\n","Turn 1: Pulled arm 1. Result: FAILURE. Arm 1 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 2: Pulled arm 3. Result: FAILURE. Arm 3 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 3: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 4: Pulled arm 2. Result: FAILURE. Arm 2 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 5: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 1, total_reward: 1). \n","Turn 6: Pulled arm 5. Result: FAILURE. Arm 5 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 7: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 2, total_reward: 2). \n","Turn 8: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 3, total_reward: 3). \n","Turn 9: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 4, total_reward: 4). \n","Turn 10: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 5, total_reward: 5). \n","Turn 11: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 6, total_reward: 6). \n","Turn 12: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 7, total_reward: 7). \n","Turn 13: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 8, total_reward: 8). \n","Turn 14: Pulled arm 5. Result: FAILURE. Arm 5 state: (#_of_pulls: 2, total_reward: 0). \n","Turn 15: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 2, total_reward: 1). \n","---------------------------------------------------------------------------\n","strategy - Basic Greed | total payout - 3 | total turns - 15\n","strategy - Upper Confidence Bound (UCB) Interval | total payout - 9 | total turns - 15\n"]}],"source":["import math\n","\n","def ucb(world, c=1.0, temp=False):\n","    \"\"\"\n","    Parameters:\n","        world (Bandit): The environment in which we are utilizing strategy\n","        c (float): Value representing the Exploration Parameter, default 1.0 as a standard\n","        temp (bool): Determines if the user desires the exploration parameter to\n","    \"\"\"\n","    while world.turn <= world.h:                           # Pull until horizon is reached\n","        curr_state = world.get_state()\n","        arm_conf = {}\n","\n","        if temp:\n","            c = c / math.sqrt(world.turn)                      # Decay c over time\n","\n","        for arm, (n, w) in enumerate(curr_state):\n","            mean_reward = w/n if n > 0 else 0                             # 'Winrate' (reward/#_of_pulls)\n","            t = world.turn\n","            # Exploration_val is set to infinity if a arm has not been chosen yet to ensure each arm is chosen at least once\n","            exploration_val = math.sqrt( math.log(t) / n ) if n > 0 else float('inf')                   # As we reach the horizon, arms that have not been pulled have a higher exploration_val\n","            uncertainty = c * exploration_val                                                           # Exploration parameter * exploration_val = uncertainty\n","            conf_val = mean_reward + uncertainty                                                        # (Exploitation + Exploration) quantified\n","            arm_conf[arm] = conf_val\n","\n","        high_val = max(arm_conf.values())                                                               # Find highest confidence value out of all arms\n","        highest_arms = [arm for arm, conf_values in arm_conf.items() if conf_values == high_val]        # Filter arms that have highest confidence value\n","        chosen_arm = np.random.choice(highest_arms)                                                     # random arm chosen in case of tie-breaker\n","        world.pull(chosen_arm)\n","\n","    world.store_strat(\"Upper Confidence Bound (UCB) Interval\")\n","\n","\n","print(\"True reward probabilities (hidden):\")\n","print(bandit_env.true_probs)\n","\n","# Simulate pulling a few arms\n","ucb(bandit_env)"]},{"cell_type":"markdown","id":"5058babe","metadata":{"jp-MarkdownHeadingCollapsed":true,"panel-layout":{"height":448.70001220703125,"visible":true,"width":100},"id":"5058babe"},"source":["## 2.1.1 Bayesian Dynamic Programming\n","\n","### Explanation of Algorithm\n","\n","The following algorithm is a Reinforcement Learning policy that utilizes Bayesian reasoning and a variant of the bellman equation to determine the optimal arm to pull that results in the highest total payoff.\n","\n","We start by using a uniform beta distribution (prior) to represent each arm's underlying true probability distribution\n","- Using observations from the environment, we update the parameters of our beta distributions to form a probability distribution that is more representative of the true underlying probability distribution (posterior) of each arm.\n","\n","To decide which arm to pull in the real environment, we calculate the total expected Value for each arm: $$\\text{Expected Value}_i = p_i \\times (R + V_{\\text{succ}}) + (1 - p_i) \\times (0 + V_{\\text{fail}})$$\n","- $R$ : immediate reward upon success\n","- $p_i$ : the probability of success (beta mean)\n","- $V_{succ}$ : the future value if successful\n","- $V_{fail}$ : the future value if not successful\n","\n","The expected value is calculated by taking a weighted sum of the two possible outcomes (success and failure), the weights are determined by taking the beta mean of the respective arm's estimated probability distribution.\n","\n","$$\\text{beta mean}=\\frac{w+1}{n+2}$$  \n","- $w$ : number of pulls that resulted in reward\n","- $n$ : total number of pulls\n","\n","Utilizing the recursive nature of the dynamic programming approach we calculate the expected value from the current turn to when $\\text{turn} = 0$, by simulating all possible actions and their respective outcomes (success, failure). Once all the recursive calls reach their base case, we choose the arm with the highest total expected value from the current turn.  \n","\n","We then interact with our environment to pull said arm, using the results we update our arm's beta distribution and calculate the total expected values again for each arm using our updated beta distribution. The agent continues until out of turns.\n","\n","### Understanding the beta distribution\n","\n","Parameters :\n","- a (alpha) : Interpreted as the \"prior count\" of successes      (evidence for the event occuring)\n","- b (beta)  : similarly, viewed as the \"prior count\" of failures (evidence against the event)\n","\n","- When we set both parameters to 1, (i.e. Beta(1,1)) we yield a uniform distribution over interval (0,1) that does not favor any probability value over another\n","\n","- Increasing alpha relative to beta shift the distribution towards the higher probability values, reflecting prior belief or evidence that successes are more likely\n","\n","- As more observations are collected, the parameters are updated (alpha increases with successes, beta increases with failures) and the distribution comes closer to the true underlying probability\n","\n","Methods of Updating after Observation:\n","- Historically, alpha would be increased by 1 when a success is observed and beta the same when a failure is observed.\n","- In the case of a non-stationary environment, we may implement a learning rate when updating the beta distribution parameters.\n","    - If the learning rate is changed over time we can model varying true probability distributions (i.e. if we slowly increase the learning rate over turns, we can \"fade out\" past observations. Modeling an environment where future observations are more significant then initial ones)\n","\n","### Future Considerations:\n","\n","The current memoization strategy persists across turns. Which in a stationary environment is valid because our memoization key (tuple(state), remaining_pulls) fully encapsulates the relevant Belief (via counts of success and failures). If we were dealing with a non-stationary environment where there exists external factors that change the dynamics, then the cached value might be out-of-date. Currently, the state (n, w) fully summarizes the underlying beliefs for the standard finite-horizon k-bandit problem.\n","\n","The algorithm is computationally expensive, with a big O notation of $O((2k)^h)$ where $k$ represents the number of arms and $h$ is our horizon. The base of the exponent is $2k$ because for reach arm we are calling two recursive calls (success, failure). In the future, pruning methods should be employed to lower the computation cost. As of now, the highest horizon we can test on the algorithm is $h=15$, any more turns result in a runtime that can last up to 40 minutes - 4 hours."]},{"cell_type":"code","execution_count":5,"id":"7fadd4af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fadd4af","executionInfo":{"status":"ok","timestamp":1737819142141,"user_tz":360,"elapsed":261134,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}},"outputId":"1727136b-4210-4c57-d757-b55cef09320b"},"outputs":[{"output_type":"stream","name":"stdout","text":["True reward probabilities (hidden):\n","[0.36284521 0.37732775 0.10454926 0.06138408 0.69978668 0.04114688]\n","Turn 1: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 1, total_reward: 1). \n","Turn 2: Pulled arm 0. Result: SUCCESS. Arm 0 state: (#_of_pulls: 2, total_reward: 2). \n","Turn 3: Pulled arm 0. Result: FAILURE. Arm 0 state: (#_of_pulls: 3, total_reward: 2). \n","Turn 4: Pulled arm 1. Result: SUCCESS. Arm 1 state: (#_of_pulls: 1, total_reward: 1). \n","Turn 5: Pulled arm 1. Result: FAILURE. Arm 1 state: (#_of_pulls: 2, total_reward: 1). \n","Turn 6: Pulled arm 2. Result: FAILURE. Arm 2 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 7: Pulled arm 3. Result: FAILURE. Arm 3 state: (#_of_pulls: 1, total_reward: 0). \n","Turn 8: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 1, total_reward: 1). \n","Turn 9: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 2, total_reward: 2). \n","Turn 10: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 3, total_reward: 3). \n","Turn 11: Pulled arm 4. Result: FAILURE. Arm 4 state: (#_of_pulls: 4, total_reward: 3). \n","Turn 12: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 5, total_reward: 4). \n","Turn 13: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 6, total_reward: 5). \n","Turn 14: Pulled arm 4. Result: SUCCESS. Arm 4 state: (#_of_pulls: 7, total_reward: 6). \n","Turn 15: Pulled arm 4. Result: FAILURE. Arm 4 state: (#_of_pulls: 8, total_reward: 6). \n","---------------------------------------------------------------------------\n","strategy - Basic Greed | total payout - 3 | total turns - 15\n","strategy - Upper Confidence Bound (UCB) Interval | total payout - 9 | total turns - 15\n","strategy - Bayesian_Bellman | total payout - 9 | total turns - 15\n"]}],"source":["def bay_rec(world):\n","    \"\"\"\n","    a bayesian policy that uses dynamic programming to find the optimal arm to pull based on expected future reward.\n","\n","    Parameters:\n","        world (bandit): the environment in which we are utilizing strategy\n","    \"\"\"\n","    def updateState(state, arm, result):\n","        \"\"\"\n","        function that returns an updated state given an arm_index and the result (success = 'True', failure = 'False')\n","\n","        Parameters:\n","            state  (list): list of tuples (n, w) representing - n (total pulls), w (successful pulls)\n","            arm     (int): an integer representing the arm_index to be updated\n","            result (bool): boolean representing success or failure of pull\n","\n","        Returns:\n","            updated state (list): the updated state after pull\n","        \"\"\"\n","        n, w = state[arm]\n","        state[arm] = (n + 1, w + result)\n","        return state\n","        #end of updateState\n","\n","    def value(val_map, state, remaining_pulls):\n","        \"\"\"\n","        Value function that recurses until base condition is reached (V(state) = 0).\n","\n","        Parameters:\n","            val_map           (dic): dictionary of world states : expected future reward\n","            state            (list): list of tuples (n, w) representing - n (total pulls), w (successful pulls)\n","            remaining_pulls   (int): the number of remaining pulls\n","\n","        Returns:\n","            value             (int): total expected future Value given pulling arm_i\n","            arm_index         (int): represents arm associated with value\n","        \"\"\"\n","        if remaining_pulls == 0:                             # Base case: if no pulls remain, return 0 and invalid arm index\n","            return (0, -1)\n","\n","        state_key = tuple(state)                             # Store (state, remaining_pulls) in val_map\n","        if (state_key, remaining_pulls) in val_map:\n","            return val_map[(state_key, remaining_pulls)]\n","\n","        best_val = -float('inf')\n","        best_arm = None                                     # Initialize best_arm with a default value\n","        for arm, (n, w) in enumerate(state):\n","\n","            #Bayesian update:\n","            p_i = (w + 1) / (n + 2)                         # Expected success probability (beta mean) for arm_i\n","\n","            # Use copies of the original state so as to preserve original state\n","            # Simulate success:\n","            state_success = updateState(state.copy(), arm, True)  # Simulate arm pull result in success\n","                                                            # Compute future Value based on success outcome\n","            value_success = value(val_map, state_success, remaining_pulls - 1)[0]\n","            # Simulate failure:\n","            state_failure = updateState(state.copy(), arm, False)  # Simulate arm pull result in failure\n","                                                            # Compute future Value based on failure outcome\n","            value_failure = value(val_map, state_failure, remaining_pulls - 1)[0]\n","                                                            # Total Expected Value\n","                                                            # For success: reward = 1 (plus future reward)\n","                                                            # For failure: reward = 0 (plus future reward)\n","            curr_val = p_i * (1 + value_success) + (1 - p_i) * (0 + value_failure)\n","\n","            if curr_val > best_val:                         # Determine the arm with the highest Total Expected Value\n","                best_val = curr_val\n","                best_arm = arm\n","\n","                                                            # Cache the computed value for this state and remaining pulls\n","        val_map[(state_key, remaining_pulls)] = (best_val, best_arm)\n","        return best_val, best_arm\n","        #end of value\n","\n","    val_map = {}                                           # Val_map, persists across turns\n","\n","    while world.turn <= world.h:                           # For every turn until horizon is reached\n","\n","        curr_state = world.get_state()                     # Get current state from environment (Updated)\n","                                                           # Compute best value and arm for the current state and remaining pulls\n","        best_val, best_arm_index = value(val_map, curr_state, world.h - world.turn + 1)\n","                                                           # Validate that best_val_arms contain at least one decision\n","        if best_arm_index is None or best_arm_index == -1:\n","            raise ValueError(\"No best arm was found. Check value function logic.\")\n","\n","        world.pull(best_arm_index)                         # interact with real environment\n","\n","    world.store_strat(\"Bayesian_Bellman\")                  # Store performance and reset Environment\n","\n","\n","print(\"True reward probabilities (hidden):\")\n","print(bandit_env.true_probs)\n","\n","bay_rec(bandit_env)"]},{"cell_type":"code","execution_count":5,"id":"4d9823af-55ec-4bc0-8769-8de8d6462cb4","metadata":{"id":"4d9823af-55ec-4bc0-8769-8de8d6462cb4","executionInfo":{"status":"ok","timestamp":1737819142142,"user_tz":360,"elapsed":26,"user":{"displayName":"Jacob Bachtarie","userId":"13476825657005784025"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"panel-cell-order":["07817029","fc6d0605","e4f22c3a","5058babe","f43f0275"],"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}